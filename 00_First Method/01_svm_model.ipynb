{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"00_unwell_well_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f abused different people young age</td>\n",
       "      <td>unwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grew dad laying top woke staed continued close...</td>\n",
       "      <td>unwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>would call mommy ask come wipe bathroom</td>\n",
       "      <td>unwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>never anything said things stayed away</td>\n",
       "      <td>unwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seventh grade became depressed staed self harming</td>\n",
       "      <td>unwell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         cleaned_text category\n",
       "5                 f abused different people young age   unwell\n",
       "6   grew dad laying top woke staed continued close...   unwell\n",
       "7             would call mommy ask come wipe bathroom   unwell\n",
       "9              never anything said things stayed away   unwell\n",
       "10  seventh grade became depressed staed self harming   unwell"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r.cleaned_text), tags=[r.category]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r.cleaned_text), tags=[r.category]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['ive', 'considered', 'cutting', 'declaring', 'independent', 'several', 'problems', 'theyre', 'paying', 'tuition', 'phone', 'plan'], tags=['unwell'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3326/3326 [00:00<00:00, 890934.67it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3326/3326 [00:00<00:00, 930389.16it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2181090.54it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2845829.27it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2862765.26it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2855148.40it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2760240.42it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2721469.98it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2833114.36it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2586254.19it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1667892.77it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2693619.44it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2774513.74it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2847571.97it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1570444.12it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2502287.91it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2582424.12it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2657191.45it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2825654.26it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2695701.47it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1777555.44it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2787820.76it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1579333.76it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2582424.12it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1563754.64it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2568161.84it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2758057.55it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 2699875.19it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1541805.38it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1562353.58it/s]\n",
      "100%|██████████| 3326/3326 [00:00<00:00, 1673896.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.09 s, sys: 1.26 s, total: 6.35 s\n",
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      unwell       0.72      0.74      0.73       718\n",
      "        well       0.73      0.71      0.72       708\n",
      "\n",
      "    accuracy                           0.72      1426\n",
      "   macro avg       0.72      0.72      0.72      1426\n",
      "weighted avg       0.72      0.72      0.72      1426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_train,y_train)\n",
    "\n",
    "y_pred = SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      unwell       0.80      0.62      0.70       718\n",
      "        well       0.69      0.85      0.76       708\n",
      "\n",
      "    accuracy                           0.73      1426\n",
      "   macro avg       0.75      0.73      0.73      1426\n",
      "weighted avg       0.75      0.73      0.73      1426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01_svm_model.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(SVM, '01_svm_model.pkl', compress=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
